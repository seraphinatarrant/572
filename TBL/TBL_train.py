'''
A script that:
- Has an initial 'annotator' which extremely dumbly labels all docs with the first label seen in training
- Takes in training data in svm-light format (and treats features as binary whether real valued or not)
- Makes a model file with the default annotation, and a list of transformations of format:
    featName from_classname to_classname net_gain
    where featName is 'if this feature is present' and net_gain is the gain of the transformation

Command to run: TBL_train.py train_data model_file min_gain
where:
train_data is the file with the training data
model_file is name of the model_file that will be generated by the trainer.
min_gain is the stopping criterion, which causes the trainer to stop iteration
'''

import argparse
from collections import defaultdict

from helpful_functions import svm_light_to_binary_TDL_features
import itertools
import numpy as np


def find_best_trans(train_records, num_transformations, trans2idx, idx2trans):
    '''

    :param train_records:
    :param num_transformations:
    :param trans2idx:
    :param idx2trans:
    :return:
    '''
    # go through training data and calc net gain for all transformations
    net_gains = np.zeros(num_transformations)
    # store a reverse dict of transformation: indices of applicable train records
    trans_record_dict = defaultdict(list)
    for index in range(len(train_records)):
        gold_l, curr_l, feat_set = train_records[index]
        for feat in feat_set:
            # if the current is the gold, penalise changing labels, if it is not the gold, reward changing to gold
            for class_label in all_class_list:
                if class_label == curr_l:
                    continue
                if curr_l == gold_l:  #penalise changing labels
                    net_gains[trans2idx[(feat, curr_l, class_label)]] -= 1
                else:  #if curr_label is not gold label reward changing to gold, else do nothing
                    if class_label == gold_l:
                        net_gains[trans2idx[(feat, curr_l, class_label)]] += 1
    # grab max
    best_idx = np.argmax(net_gains)
    best_trans, best_gain = idx2trans[best_idx], net_gains[best_idx]

    return best_trans, best_gain


def apply_trans_to_data(records, trans, record_transformations=False, applied_transformations=None):
    '''
    modifies a set of training records in place based on a given transformation
    :param records: a nested list of form: [[gold class_label, current class label, set of binary features present in sample],...]
    :param trans: a transformation of form: feat from_label to_label
    :param record_transformations: boolean
    :param applied_transformations: list to store applied transformations, coindexed with records
    :return: None - modifies training records in place (and applied transformations list as well if applicable)
    '''
    trans_feat, from_label, to_label = trans
    for i in range(len(records)):
        gold, curr, feat_set = records[i]
        if trans_feat not in feat_set or curr != from_label:
            continue
        else:
            records[i][1] = to_label
            if record_transformations:
                applied_transformations[i].append(trans)


def train_TBL(train_records, all_class_list, all_feat_list, min_gain):
    '''

    :param train_records: a nested list of form
    [[gold class_label, dummy class label, set of binary features present in sample],...]
    :param all_class_list: a list of all classes
    :param all_feat_list: a list of all features
    :param min_gain: the minimum gain (a hyperparameter) for early stopping
    :return: a TBL model file in the format: first line is init_class, following are transformations in order, format:
    feat from_label to_label gain
    '''
    # initialise the possible transformations
    model = [all_class_list[0]]  # first label in all_class_list is the default label (based on how I made the function)
    t_idx = 0
    trans2idx, idx2trans = {}, {}
    for feature in all_feat_list:
        # this will include all cases where class i == class j, but not a big deal since the if statement gets rid of those
        for from_label, to_label in itertools.product(all_class_list, all_class_list):
            trans = (feature, from_label, to_label)
            trans2idx[trans] = t_idx
            idx2trans[t_idx] = trans
            t_idx += 1

    # init best trans and best gain
    best_trans, best_gain = find_best_trans(train_records, t_idx, trans2idx, idx2trans)  # t_idx is number of transformations
    while best_gain >= min_gain:
        #print(best_trans, best_gain)
        #store best trans
        model.append('{} {}'.format(' '.join(best_trans), best_gain))
        #apply trans (returns training data)
        apply_trans_to_data(train_records, best_trans)
        #recalc
        best_trans, best_gain = find_best_trans(train_records, t_idx, trans2idx, idx2trans)

    return model



if __name__ == "__main__":
    p = argparse.ArgumentParser()
    p.add_argument('train_filename', help='the filename with the training data, svm-light format')
    p.add_argument('model_filename', help='the filename to output the model to')
    p.add_argument('min_gain', type=int, help='the min gain to allow in training')
    args = p.parse_args()

    train_records, all_class_list, all_feat_list = svm_light_to_binary_TDL_features(args.train_filename)
    TBL_model = train_TBL(train_records, all_class_list, all_feat_list, args.min_gain)
    # write model
    with open(args.model_filename, 'w') as mfile:
        mfile.write('\n'.join(TBL_model))
